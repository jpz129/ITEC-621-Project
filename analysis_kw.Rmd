---
title: "analyis_kw"
author: "Katherine Westcott"
date: "3/30/2021"
output: pdf_document
---

```{r imports} 

# set up
library(tidyverse)
library(dplyr)
library(ggplot2)
library(glmnet) # Contains functions for Ridge and LASSO

proj.data.orig <- read_csv("data/fixed_project_data.csv")[,c(1,2,3,4,5,8,11,14,17,20,23,26,29,32,35,41,42)]
proj.data.orig <- na.omit(proj.data.orig)

inc <- read_csv("/Users/kw/Desktop/Project/ITEC-621-Project/data/Unemployment.csv")
inc <- na.omit(inc)

popb <- read_csv("/Users/kw/Desktop/Project/ITEC-621-Project/data/PopulationEstimates.csv")

```

#### Adding county income and population data to project_data.csv

Median income for 2019 from

https://www.ers.usda.gov/data-products/county-level-data-sets/

```{r income}

inc <- inc %>%
  select(fips_txt, Median_Household_Income_2019, Med_HH_Income_Percent_of_State_Total_2019) %>%
  rename(county_fips = "fips_txt")

# Join

full <- inc %>%
  full_join(proj.data.orig, by = "county_fips")

```

Population as of 2019 from 

https://www.ers.usda.gov/data-products/county-level-data-sets/

**It would be good to find county land area data so we could use population density**

```{r Population}

# USDA Population estimate for 2019
popa <- popb %>%
  select(FIPStxt, POP_ESTIMATE_2019) 

pop <- popa %>%
  rename(county_fips = "FIPStxt")

pop$county_fips <- sub("^0+", "", pop$county_fips)
pop$county_fips <- as.double(pop$county_fips)

# Join

full <- pop %>%
  inner_join(full, by = "county_fips")

```


Cases and deaths per 100 people 

```{r cd per hundred}

# Create cases and deaths per capita

full <- full %>%
  mutate(cases_per_1000 = (cases/(POP_ESTIMATE_2019/1000))) %>%
  mutate(deaths_per_1000 = (deaths/(POP_ESTIMATE_2019/1000)))

```


inspect dimensions and missing values 

```{r dimensions}

length(unique(full$date))
length(unique(full$county_fips))
length(unique(full$state))

colSums(is.na(full))

dim(full)

```



Add land area and make population density 

```{r land imp}

full <- na.omit(full)
la <- read_csv("data/landarea.csv")[,c(2,4)]

la <- la %>%
  rename(county_fips = "STCOU") %>%
  rename(county_land_area = "LND010190D")

la$county_fips <- sub("^0+", "", la$county_fips)

la$county_fips <- as.double(la$county_fips)

head(la)
head(full)

```

```{r join la}

full <- full %>%
  left_join(la, by = "county_fips")

full <- full %>%
  mutate(pop_density_sqmi = (POP_ESTIMATE_2019/county_land_area))

head(full)
dim(full)


colSums(is.na(full))

```


```{r regions}

reg <- read_csv("https://raw.githubusercontent.com/kwestcott10/stat612.beeproject/main/regions.csv")

tolow <- function(x) {
  substr(x, 1, 1) <- tolower(substr(x, 1, 1))
  x
}

reg$State <- tolow(reg$State)

regs <- reg %>%
  rename(state = "State") %>%
  select(!`State Code`)

full<- full %>%
  left_join(regs, by = "state")

write.csv(full, file = "/Users/kw/Desktop/Project/ITEC-621-Project/data/Project_Data_final.csv")

```

Regions:
1 = Midwest
2 = Northeast
3 = South
4 = West

Divisions:
1 = East North Central
2 = East South Central
3 = Middle Atlantic
4 = Mountain
5 = New England
6 = Pacific
7 = South Atlantic
8 = West North Central
9 = West South Central


### FULL DATASET = "Project_Data_final.csv", called "full" in this file.

### Descriptive plots, etc

median of the median house price across each division, plotted against date. Dot sizes represent the mean number of cases per capita across that division. 


```{r Cases by division}

div_avg <- full %>%
  group_by(Division, date) %>%
  summarize(div_avg_cases_per_1000 = mean(cases_per_1000), date = date, div_median_listing_price = median(median_listing_price))

ggplot(data = div_avg) +
  geom_point(mapping = aes(x = date, y = div_median_listing_price, color = Division, size = div_avg_cases_per_1000), se = FALSE) +
  scale_size_continuous(range = c(1,8))
  theme_minimal()

```

## Predictive Analysis section

### Simple OLS

```{r ols}

lm.fit <- lm(median_listing_price_log ~ ., data = data_log)
summary(lm.fit)

```


### Assumption: evenly distributed residuals

#### Check for heteroskedasticity

```{r bp test}

library(lmtest)
bptest(lm.fit)

plot(lm.fit, which = 1)

```

This looks good 

#### Check for serial correlation

**does this test hold without the date variable?**

```{r dw}

library(lmtest)

dwtest(lm.fit)

```
DW = 0.178 << 2 --> High positive serial correlation



```{r}
#data_log$date <- as.character(data_log$date)
#data_log$date <- parse_date(data_log$date, format = "%Y-%M-%d")
```

### KNN

```{r knn}

library(FNN)

#Can I do knn with Division and date?
data_log_knn <- data_log[,c(3,4,6,8,9,11)]
as.data.frame(data_log_knn)

#Should set.seed again? or not  
set.seed(123)
train <- sample(nrow(data_log_knn),0.8*nrow(data_log_knn))

data_train <- data_log_knn[train,-6]
data_train_results <- data_log_knn[train,6]

data_test <- data_log_knn[-train,-6]
data_test_results <- data_log_knn[-train,6]

knn.mod <- knn(data_train, data_test, data_train_results, k=3)

```
```{r best k}

# Check to see if there is a better k
best.k <- -1
error.rate <- -1
best.error.rate <- 99999999
for (i in 3:100) {
  knn.mod <- knn(data_train, data_test, data_train_results, k = i)
  error.rate <- 1-(sum(knn.mod == data_test_results) / length(data_test_results))
  if (error.rate < best.error.rate) {
    best.k <- i
    best.error.rate <- error.rate
  }
}
print(paste("The optimal value of k is",best.k,"with an overall error rate of",best.error.rate))

```


```{r knn results}
accuracy <- sum(knn.mod == data_test_results) / length(data_test_results)

print(paste("the knn for classification accuracy is",(accuracy*100),"%"))

conf.mat <- table(knn.mod, data_test_results)

conf.mat
```


Pretty good!

```{r more stats}

TruH <- conf.mat[1,1] # True highs
TruM <- conf.mat[2,2] # True meds
TruL <- conf.mat[3,3] # True lows
 
FalH <- sum(conf.mat[1,2:3])
FalM <- sum(conf.mat[2,1], conf.mat[2,3])
FalL <- sum(conf.mat[3,1:2])

TotH <- sum(conf.mat[1,])
TotM <- sum(conf.mat[2,])
TotL <- sum(conf.mat[3,])
Tot = TotH + TotM + TotL

Error.Rate <- (FalH + FalM + FalL) / Tot
Error.Rate # Check it out

Accuracy.Rate <- (TruH + TruM + TruL) / Tot
Accuracy.Rate # Same as the proportion calculated above 

# Proportion of correct "Highs"
HProp <- TruH / TotH
# Proportion of correct "Mediums"
MProp <- TruM / TotM
# Proportion of correct "Lows"
LProp <- TruL / TotL

# Should I break it up into smaller matrices and find sensitivity and specificity? 

knn.stats <- c(Accuracy.Rate, Error.Rate, HProp, MProp, LProp)

names(knn.stats) <- 
    c("Accuracy Rate", "Error Rate", "Prop of Correct Highs", "Prop of Correct Mediums", "Prop of Correct Lows")

knn.stats

```

Adding date

```{r date}

library(lubridate)

data_log_datesep <- data_log %>% 
  mutate(year = year(date), month = month(date), day = mday(date))

data_log_datesep$month_consec <- 0
for (i in 1:nrow(data_log_datesep)) {
  if (data_log_datesep$year[i] == "2020") {
    data_log_datesep$month_consec[i] <- data_log_datesep$month[i]
  } else if (data_log_datesep$year[i] == "2021") {
    data_log_datesep$month_consec[i] <- (data_log_datesep$month[i] + 12)
  }
}

```


```{r}

#Can I do knn with Division and date?
data_log_knn <- data_log_datesep[,c(3,4,6,8,9,11,15)]
as.data.frame(data_log_knn)

#Should set.seed again? or not  
set.seed(123)
train <- sample(nrow(data_log_knn),0.8*nrow(data_log_knn))

data_train <- data_log_knn[train,-6]
data_train_results <- data_log_knn[train,6]

data_test <- data_log_knn[-train,-6]
data_test_results <- data_log_knn[-train,6]

knn.mod.date.consec <- knn(data_train, data_test, data_train_results, k=12)

```

```{r results consecutive date}

accuracy <- sum(knn.mod.date.consec == data_test_results) / length(data_test_results)

print(paste("the knn for classification accuracy is",(accuracy*100),"%"))

conf.mat.consec <- table(knn.mod.date.consec, data_test_results)

conf.mat.consec

```

```{r regular date}

#Can I do knn with Division and date?
data_log_knn <- data_log_datesep[,c(3,4,6,8,9,11,12,13,14)]
as.data.frame(data_log_knn)

#Should set.seed again? or not  
set.seed(123)
train <- sample(nrow(data_log_knn),0.8*nrow(data_log_knn))

data_train <- data_log_knn[train,-6]
data_train_results <- data_log_knn[train,6]

data_test <- data_log_knn[-train,-6]
data_test_results <- data_log_knn[-train,6]

knn.mod.date <- knn(data_train, data_test, data_train_results, k=12)

```

```{r results regular date}

accuracy <- sum(knn.mod.date == data_test_results) / length(data_test_results)

print(paste("the knn for classification accuracy is",(accuracy*100),"%"))

conf.mat.date <- table(knn.mod.date, data_test_results)

conf.mat.date

```

