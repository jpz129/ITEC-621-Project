---
title: "analyis_kw"
author: "Katherine Westcott"
date: "3/30/2021"
output: pdf_document
---

```{r imports} 

# set up
library(tidyverse)
library(dplyr)
library(ggplot2)

pan.hous <- read_csv("data/project_data.csv")[,c(2,3,4,5,6,7,8,11,35)]
pan.hous <- na.omit(pan.hous)

inc <- read_csv("/Users/kw/Desktop/Project/ITEC-621-Project/data/Unemployment.csv")
inc <- na.omit(inc)

popb <- read_csv("/Users/kw/Desktop/Project/ITEC-621-Project/data/PopulationEstimates.csv")

```

#### Adding county income and population data to project_data.csv

Median income for 2019 from

https://www.ers.usda.gov/data-products/county-level-data-sets/

```{r income}

inc <- inc %>%
  select(fips_txt, Median_Household_Income_2019, Med_HH_Income_Percent_of_State_Total_2019) %>%
  rename(county_fips = "fips_txt")

# Join

full <- inc %>%
  full_join(pan.hous, by = "county_fips")

```

Population as of 2019 from 

https://www.ers.usda.gov/data-products/county-level-data-sets/

**It would be good to find county land area data so we could use population density**

```{r Population}

# USDA Population estimate for 2019
popa <- popb %>%
  select(FIPStxt, POP_ESTIMATE_2019) 

pop <- popa %>%
  rename(county_fips = "FIPStxt")

pop$county_fips <- sub("^0+", "", pop$county_fips)
pop$county_fips <- as.double(pop$county_fips)

# Join

full <- pop %>%
  inner_join(full, by = "county_fips")

```


Cases and deaths per 100 people 

```{r cd per hundred}

# Create cases and deaths per capita

full <- full %>%
  mutate(cases_per_1000 = (cases/(POP_ESTIMATE_2019/1000))) %>%
  mutate(deaths_per_1000 = (deaths/(POP_ESTIMATE_2019/1000)))

```


inspect dimensions and missing values 

```{r dimensions}

length(unique(full$date))
length(unique(full$county_fips))
length(unique(full$state))

colSums(is.na(full))

dim(full)

```

Omit na

```{r missing}

full <- na.omit(full)

# find out which 4 counties are missing 

big.fips <- unique(pan.hous$county_fips)
small.fips <- unique(full$county_fips)

missing.vect <- big.fips[! big.fips %in% small.fips] 


#Missing counties are: jefferson, st. charles, orleans and st. tammany LA
missing.vect

pan.hous %>%
  filter(county_fips == "22103")

dim(full)

```

Add land area and make population density 

```{r land imp}

la <- read_csv("data/landarea.csv")[,c(2,4)]

la <- la %>%
  rename(county_fips = "STCOU") %>%
  rename(county_land_area = "LND010190D")

la$county_fips <- sub("^0+", "", la$county_fips)

la$county_fips <- as.double(la$county_fips)

head(la)
head(full)

```

```{r join la}

full <- full %>%
  left_join(la, by = "county_fips")

full <- full %>%
  mutate(pop_density_sqmi = (POP_ESTIMATE_2019/county_land_area))

head(full)
dim(full)

write.csv(full, file = "/Users/kw/Desktop/Project/ITEC-621-Project/data/Project_Data_final.csv")

colSums(is.na(full))

```


#### Mirroring some of Pei's models with the new predictors 


```{r regions}

reg <- read_csv("https://raw.githubusercontent.com/kwestcott10/stat612.beeproject/main/regions.csv")

tolow <- function(x) {
  substr(x, 1, 1) <- tolower(substr(x, 1, 1))
  x
}

reg$State <- tolow(reg$State)

regs <- reg %>%
  rename(state = "State")

full<- full %>%
  left_join(regs, by = "state")

```

Regions:
1 = Midwest
2 = Northeast
3 = South
4 = West

Divisions:
1 = East North Central
2 = East South Central
3 = Middle Atlantic
4 = Mountain
5 = New England
6 = Pacific
7 = South Atlantic
8 = West North Central
9 = West South Central

```{r corr}

full.sm <- full[,c(3,6,7,8,9,10,11,12,14,18)]
head(full.sm)
colSums(is.na(full.sm))

library(corrplot)

#full.sm$Region <- as.numeric(as.factor(full.sm$Region))
full.sm$state <- as.numeric(as.factor(full.sm$state))
full.sm$county <- as.numeric(as.factor(full.sm$county))
full.sm$pop_density_sqmi <- as.numeric(as.factor(full.sm$pop_density_sqmi))
full.sm$Division <- as.numeric(as.factor(full.sm$Division))

mat <- as.matrix(full.sm)
corr <- cor(mat)

corrplot(corr, method = "shade")
corrplot(corr, method = "number", tl.cex = 0.3)

# make 3 sections: low, avg, high
#sd(full$median_listing_price)


```

### Descriptive plots, etc

median of the median house price across each division, plotted against date. Dot sizes represent the mean number of cases per capita across that division. 

```{r Cases by division}

div_avg <- full %>%
  group_by(Division, date) %>%
  summarize(div_avg_cases_pc = mean(cases_per_100), date = date, div_median_listing_price = median(median_listing_price))

ggplot(data = div_avg) +
  geom_point(mapping = aes(x = date, y = div_median_listing_price, color = Division, size = div_avg_cases_pc), se = FALSE) +
  scale_size_continuous(range = c(1,8))
  theme_minimal()


```








### Simple OLS

```{r ols}

lm.fit <- lm(average_listing_price ~ ., data = full.sm)
summary(lm.fit)

```

### Assumption: evenly distributed residuals

```{r bp test}

library(lmtest)
bptest(lm.fit)

plot(lm.fit, which = 1)

```

There is certainly heteroskedasticity in this model.

Fit a wls

```{r wls}

abs.res <- abs(residuals(lm.fit))
lm.preds <- fitted(lm.fit)
lm.abs.res <- lm(abs.res ~ lm.preds)

head(fitted(lm.abs.res), n = 10)

wts <- 1/fitted(lm.abs.res)^2

wls.fit <- lm(average_listing_price ~ ., data = full.sm, weights = wts)
summary(wls.fit)

```

#### Check for serial correlation

**does this test hold without the date variable?**

```{r dw}

library(lmtest)

dwtest(lm.fit)

plot(full.sm$Median_Household_Income_2019, lm.fit$residuals)

```

DW = 0.323 << 2 --> High positive serial correlation

### Assumption: predictors independent 




